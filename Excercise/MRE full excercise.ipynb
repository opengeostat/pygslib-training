{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mineral Resource estimation with Python\n",
    "\n",
    "Welcome to the **world of Python**. \n",
    "\n",
    "This is a long exercise. Here you will:\n",
    " - create drillholes, and a geological model\n",
    " - tag drillholes with domain\n",
    " - create a block model and calculate percentage of mineralized material\n",
    " - composite\n",
    " - do statistical analysis (variography not implemented but explained)\n",
    " - interpolate grade and validate interpolations\n",
    " - Report resources\n",
    " \n",
    " All this using Python and Paraview. The figure below shows some of the outcomes of this exercise\n",
    " \n",
    " <img src = 'figures/fig1.JPG'>\n",
    " \n",
    " Before we start let's do a quick review of Jupyter (this IDE) and python\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercise 1 Introduction to Jupyter and Python\n",
    "\n",
    "Familiarizing yourself with Jupyter:\n",
    " - Open jupyter notebook (you are already in)\n",
    " - Rename the file as \"my MRE using python\" (make a copy to keep the original file unchanged)\n",
    " - Add Markdown cell and type some comments (double click here! to see the markdown)\n",
    "\n",
    "Python introduction\n",
    " - Import libraries  \n",
    " - Import drillhole data located in folder `data/` into pandas DataFrames\n",
    " - Explore the data using pandas and plot collar location \n",
    " - Create a column of $log_{e}(Au)$ in table assays \n",
    "\n",
    "Note: **Double click here to see the code of the cell. ** This will give you an idea about writing comments in markdown\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# these are magic commands used to control Jupyter's behavior \n",
    "%gui qt                            \n",
    "%matplotlib inline \n",
    "#%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries \n",
    "import pandas as pd               # this imports pandas as pd\n",
    "import pygslib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some tips:\n",
    " - use key `TAB` to do autocompletion. Example type `pd.rea` and press `TAB`\n",
    " - use keys `CHIFT+TAB` to see help  \n",
    " - use type `?<python object>` and `Enter` to print help, also `help(<python object>)`, and also `print(<python object>.__doc__)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Import drillhole data located in folder data/ into pandas dataframes\n",
    "collar = pd.read_csv('data/collar.csv')\n",
    "survey = pd.read_csv('data/survey.csv')\n",
    "assay  = pd.read_csv('data/assay.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Explore the data using pandas and plot collar location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# plot first few lines\n",
    "collar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# see column types and names\n",
    "print ('*** Table Collar ***\\n', collar.dtypes)\n",
    "print ('*** Table Survey ***\\n', survey.dtypes)\n",
    "print ('*** Interval Table ***\\n', assay.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "<b>Important!</b> In PyGSLIB column names and types are prescribed, like in Datamine. Each table type is supposed to have certain \"system\" columns, as in this example. Names are case-sensitive, and \"system\" compulsory names are usually in upper case.  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary stats for Au \n",
    "assay['Au'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Au log\n",
    "assay['log(Au)'] = np.log(assay['Au']+0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick histogram of Au and Au log\n",
    "assay[['Au', 'log(Au)']].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot collars \n",
    "collar.plot.scatter(x='XCOLLAR', y= 'YCOLLAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paraview\n",
    "Plot collars in Paraview: \n",
    "- drag and drop collar table into Paraview\n",
    "- use filter `table to points`\n",
    "- use gaussian points visualization\n",
    "\n",
    "![figures/fig2.JPG](figures/fig2.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Drillholes \n",
    "\n",
    "Exploratory data analysis\n",
    " - Create a drillhole object\n",
    " - Add drillhole intervals\n",
    " - Validate drillhole object and interval table\n",
    " - Desurvey \n",
    " - Export drillhole as vtk file and identify mineralization (tip use `tube filter`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create drillhole object\n",
    "mydholes = pygslib.drillhole.Drillhole(collar, survey)   # note that Drillhole is a class and mydholes is an instance of this class\n",
    "\n",
    "# add interval table\n",
    "mydholes.addtable(table = assay, table_name = 'assay')  # here we are using named parameters, the order is not important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate\n",
    "mydholes.validate()\n",
    "mydholes.validate_table('assay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if no red box then the drillhole is ok, you are good to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tables are stored in a list of panda arrays named table, within drillhole object\n",
    "mydholes.table['assay'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydholes.desurvey('assay')     # desurvey \n",
    "mydholes.table['assay'].head() # see what happens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - xm,ym,zm are the coordinates of the middle interval\n",
    " \n",
    "you also have endpoints\n",
    "\n",
    "- xb,yb,zb are the coordinates of the beginning of the interval \n",
    "- xe,ye,ze are the coordinates of the end of the interval\n",
    "\n",
    "Endpoints are optional but are required to generate vtk files. To disable endpoints use `endpoints=False` in function `desurvey()`\n",
    "\n",
    "inclinations of intervals, azm, and dip, are also calculated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export drillhole as vtk file and identify mineralization (tip use tube filter)\n",
    "mydholes.intervals2vtk(table_name= 'assay', filename= 'assay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the file in Paraview (drag and drop). Use log scale to visualize Au grades, with interval from 0.01 to 5 g/t. It may look like this:  \n",
    "\n",
    "<img src = 'figures/fig3.JPG' width = '50%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: \n",
    "\n",
    "The mineralization looks like a simple horizon. It makes sense modeling this as surfaces. Because it is so simple, we can use queries from pandas to extract contact points. \n",
    "\n",
    "Modeling\n",
    " - Label drillhole intervals with domain (tip use key composite to simplify)\n",
    " - Extract contact points to model surfaces (tip can use queries in pandas or manual selection in Paraview)\n",
    " - Create contact surfaces and topo interpolating with Rbf (vtkPolyData with open surface triangulations)\n",
    " - Define working region extent\n",
    "\n",
    "Create solids\n",
    " - Convert vtkPolyData surfaces to implicit functions  \n",
    " - Model solids using:\n",
    "     - cutting tool or \n",
    "     - evaluating distances\n",
    "\n",
    "Tag drillhole data\n",
    " - Tag drillhole data with domain (optional in this case)\n",
    " - composite drillholes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) create domain using pandas filters (see help on command .loc for more info)\n",
    "mydholes.table['assay']['CMPDOM'] = 0   # new field all =  0 \n",
    "mydholes.table['assay'].loc[mydholes.table['assay']['Au']>0.01,'CMPDOM'] = 1 # we select all intervals with Au> 0.01 and set CMPDOM = 1\n",
    "\n",
    "mydholes.table['assay'][['BHID', 'FROM', 'TO', 'Au', 'CMPDOM']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) composite by domain  \n",
    "mydholes.key_composite(table_name='assay', key_name='CMPDOM', variable_name= 'Au', new_table_name = 'litho',overwrite = True)\n",
    "mydholes.table['litho'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compositing removes coordinates, you need to desurvey again\n",
    "mydholes.desurvey('litho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can get coordinates of hanging and footwall\n",
    "hw = mydholes.table['litho'].loc[mydholes.table['litho']['CMPDOM']==1, ['xb','yb','zb']]\n",
    "fw = mydholes.table['litho'].loc[mydholes.table['litho']['CMPDOM']==1, ['xe','ye','ze']]\n",
    "\n",
    "# save the points to review it in Paraview\n",
    "hw.to_csv('hw.csv', index= False)\n",
    "fw.to_csv('fw.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may see something like this in Paraview\n",
    "\n",
    "<img src = 'figures/fig4.JPG' width = '50%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining working region and modeling surfaces\n",
    "\n",
    "Surfaces and solids are generated within a working region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define working region extent and point spacing\n",
    "xorg = -10\n",
    "yorg = -10\n",
    "zorg = -10\n",
    "dx = 5\n",
    "dy = 5\n",
    "dz = 5\n",
    "nx = 40\n",
    "ny = 44\n",
    "nz = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate vtk open surfaces\n",
    "# the output is a mesh in vtk format, and node coordinates\n",
    "topo_pd,x_topo,y_topo,z_topo = pygslib.vtktools.rbfinterpolate( x=mydholes.collar['XCOLLAR'].values,\n",
    "                                                                y=mydholes.collar['YCOLLAR'].values,\n",
    "                                                                z=mydholes.collar['ZCOLLAR'].values,\n",
    "                                                                xorg=xorg, yorg=yorg,dx=dx,dy=dy,nx=nx,ny=ny)\n",
    "\n",
    "hw_pd,x_hw,y_hw,z_hw  = pygslib.vtktools.rbfinterpolate(  x=hw['xb'].values,\n",
    "                                                       y=hw['yb'].values,\n",
    "                                                       z=hw['zb'].values,\n",
    "                                                       xorg=xorg, yorg=yorg,dx=dx,dy=dy,nx=nx,ny=ny)\n",
    "\n",
    "fw_pd,x_hw,y_hw,z_hw  = pygslib.vtktools.rbfinterpolate(  x=fw['xe'].values,\n",
    "                                                       y=fw['ye'].values,\n",
    "                                                       z=fw['ze'].values,\n",
    "                                                       xorg=xorg, yorg=yorg,dx=dx,dy=dy,nx=nx,ny=ny)\n",
    "\n",
    "\n",
    "# and save the surfaces\n",
    "pygslib.vtktools.SavePolydata(topo_pd, 'topo')\n",
    "pygslib.vtktools.SavePolydata(hw_pd, 'hw')\n",
    "pygslib.vtktools.SavePolydata(fw_pd, 'fw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = 'figures/fig5.JPG' width = '50%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a region,\n",
    "\n",
    "A region is a vtk grid with regularly spacing points and optionally contact points (snapping). The grid has octahedron cells generated with Delaunay 3D triangulation. This takes a bit of time, be patient... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a grid (a box, we cut to generate geology). We can generate a grid or tetras with surface point included to emulate snapping \n",
    "region = pygslib.vtktools.define_region_grid(xorg, yorg, zorg, dx, dy,  dz, nx, ny, nz, snapping_points = [topo_pd,hw_pd,fw_pd])\n",
    "pygslib.vtktools.SaveUnstructuredGrid(region, \"region\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create solids\n",
    "Solids will be created cutting or evaluating distances. Both involve implicit functions and require implicit surfaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert vtkPolyData surfaces to implicit functions\n",
    "impl_topo = pygslib.vtktools.implicit_surface(topo_pd)\n",
    "impl_hw = pygslib.vtktools.implicit_surface(hw_pd)\n",
    "impl_fw = pygslib.vtktools.implicit_surface(fw_pd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can generate solids\n",
    "\n",
    "Option (a) using cutter (clip), we basically slice a region with implicit surfaces. This is not very stable, we prefer option b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model below topo\n",
    "#topo_region_c,topo_solid_c = pygslib.vtktools.clip_with_surface(region, implicit_surface = impl_topo, how='outside')\n",
    "#topo_region_c,topo_solid_c = pygslib.vtktools.clip_with_surface(topo_region_c, implicit_surface = impl_hw, how='inside')\n",
    "#pygslib.vtktools.SavePolydata(topo_solid_c, 'topo_solid_c')\n",
    "\n",
    "# get model between hw and fw\n",
    "#d1_region_c,d1_solid_c = pygslib.vtktools.clip_with_surface(region, implicit_surface = impl_hw, how='outside')\n",
    "#d1_region_c,d1_solid_c = pygslib.vtktools.clip_with_surface(d1_region_c, implicit_surface = impl_fw, how='inside')\n",
    "#pygslib.vtktools.SavePolydata(d1_solid_c, 'd1_solid_c')\n",
    "\n",
    "# get model below  fw\n",
    "#base_region_c,base_solid_c = pygslib.vtktools.clip_with_surface(region, implicit_surface = impl_fw, how='outside')\n",
    "#pygslib.vtktools.SavePolydata(base_solid_c , 'base_solid_c')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option (b) evaluating a region with implicit surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate surfaces\n",
    "#below topo\n",
    "region,topo_d = pygslib.vtktools.evaluate_region(region, implicit_func = impl_topo, func_name='topo_d', invert=False, capt = -10000)\n",
    "#above hanging wall\n",
    "region, hw_u = pygslib.vtktools.evaluate_region(region, implicit_func = impl_hw, func_name='hw_u', invert=True, capt = -10000)\n",
    "#below hanging wall\n",
    "region, hw_d = pygslib.vtktools.evaluate_region(region, implicit_func = impl_hw, func_name='hw_d', invert=False, capt = -10000)\n",
    "#above footwall\n",
    "region, fw_u = pygslib.vtktools.evaluate_region(region, implicit_func = impl_fw, func_name='fw_u', invert=True, capt = -10000)\n",
    "#below footwall\n",
    "region, fw_d = pygslib.vtktools.evaluate_region(region, implicit_func = impl_fw, func_name='fw_d', invert=False, capt = -10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we can use regions to:\n",
    " - do boolean operations \n",
    " - extract surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create intersection between hanging wall and foot wall\n",
    "dom1= np.minimum(hw_d, fw_u)\n",
    "region = pygslib.vtktools.set_region_field(region, dom1, 'dom1')\n",
    "# extract surface\n",
    "dom1_poly = pygslib.vtktools.extract_surface(region,'dom1')\n",
    "# Save surface\n",
    "pygslib.vtktools.SavePolydata(dom1_poly, 'dom1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create intersection between topo and hanging wall\n",
    "dom_topo= np.minimum(topo_d, hw_u)\n",
    "region = pygslib.vtktools.set_region_field(region, dom_topo, 'dom_topo')\n",
    "# extract surface\n",
    "dom_topo_poly = pygslib.vtktools.extract_surface(region,'dom_topo')\n",
    "# Save surface\n",
    "pygslib.vtktools.SavePolydata(dom_topo_poly, 'dom_topo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not boolean required below fw\n",
    "# extract surface\n",
    "dom_fw_poly = pygslib.vtktools.extract_surface(region,'fw_d')\n",
    "# Save surface\n",
    "pygslib.vtktools.SavePolydata(dom_fw_poly, 'dom_fw')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drillholes tagging\n",
    "\n",
    "Tagging assigns a code to drillholes within a domain, usually defined by wireframes. \n",
    "\n",
    "We already have drillhole tags but we show you how to do it. You will need to do this if wireframes are created manually or imported. The functions that can tag are:\n",
    "\n",
    " - `pygslib.vtktools.pointinsolid()` This is the preferred way, requires closed solids. \n",
    " - `pygslib.vtktools.evaluate_implicit_points()`. New and experimental, uses implicit functions\n",
    " - `pygslib.vtktools.pointquering()`. High level function.\n",
    " - `pygslib.vtktools.vtk_raycasting()`. Low level function.  Used by pointinsolid()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating array to tag samples in domain1\n",
    "inside1=pygslib.vtktools.pointinsolid(dom1_poly, \n",
    "                       x=mydholes.table['assay']['xm'].values, # .values this extracts numpy array from pandas\n",
    "                       y=mydholes.table['assay']['ym'].values, \n",
    "                       z=mydholes.table['assay']['zm'].values)\n",
    "\n",
    "# creating a new domain field \n",
    "mydholes.table['assay']['Domain1']=inside1.astype(int)\n",
    "\n",
    "# first 3 rows of a subtable\n",
    "mydholes.table['assay'].loc[(mydholes.table['assay']['BHID']=='0') & (mydholes.table['assay']['FROM']>50), \n",
    "                            ['BHID', 'FROM', 'TO', 'Domain1', 'CMPDOM', 'Au']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 4: Block models\n",
    "\n",
    "Create a block model with definition: \n",
    "```\n",
    "xorg = 0\n",
    "yorg = 0\n",
    "zorg = 0\n",
    "dx = 10\n",
    "dy = 10\n",
    "dz = 10\n",
    "nx = 18\n",
    "ny = 20\n",
    "nz = 15\n",
    "\n",
    "```\n",
    "\n",
    "Calculate the percent or proportion inside domain 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an empty model\n",
    "mymodel = pygslib.blockmodel.Blockmodel(xorg = 0,\n",
    "                                        yorg = 0,\n",
    "                                        zorg = 0,\n",
    "                                        dx = 10,\n",
    "                                        dy = 10,\n",
    "                                        dz = 10,\n",
    "                                        nx = 36/2,\n",
    "                                        ny = 40/2,\n",
    "                                        nz = 30/2)\n",
    "\n",
    "#generate blocks and calculate percent in domain 1\n",
    "modelvtk = mymodel.fillwireframe(dom1_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model is now created and stored in the property bmtable as pandas dataframe. \n",
    "# The property _in is the proportion inside domain 1\n",
    "mymodel.bmtable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model output is a vtkimageData, you can save it to see in Paraview\n",
    "pygslib.vtktools.SaveImageData(modelvtk, 'bmodel') # also mymodel.blocks2vtkImageData\n",
    "print(modelvtk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets keep only blocks that touch the cell by removing blocks with zero proportion\n",
    "mymodel.set_blocks (mymodel.bmtable[mymodel.bmtable['__in']>0])\n",
    "\n",
    "# and save the model as vtkunestructured grid\n",
    "mymodel.blocks2vtkUnstructuredGrid('bmodel') # this will have extension vtu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a problem with the resolution, we need smaller blocks\n",
    "\n",
    "<img src = 'figures/fig6.JPG' width = '50%' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go back and use 5m blocks and twice the number of blocks in each direction to get something like this \n",
    "\n",
    "<img src = 'figures/fig7.JPG' width = '50%' >\n",
    "\n",
    "Hint: in Paraview, right click on the model (pipeline tab) and reload file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 5: Stats and compositing\n",
    "Now is the time to get ready for interpolation\n",
    "\n",
    "In this exercise you will do:\n",
    "- compositing intervals\n",
    "- declustering \n",
    "- statistical analysis \n",
    "- variography"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compositing\n",
    "\n",
    "It is not required in this case, all the samples are 1m interval. We show you how to do this. Compositing in pygslib is a bit different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we remove Au outside domain 1\n",
    "mydholes.table['assay']['Au_D1'] = mydholes.table['assay']['Au']\n",
    "mydholes.table['assay'].loc[(mydholes.table['assay']['CMPDOM']!=1) , 'Au_D1'] = None\n",
    "\n",
    "# see results\n",
    "# first n rows of a table\n",
    "mydholes.table['assay'].loc[(mydholes.table['assay']['BHID']=='0') & (mydholes.table['assay']['FROM']>50), \n",
    "                            ['BHID', 'FROM', 'TO', 'Domain1', 'CMPDOM', 'Au','Au_D1']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then you composite\n",
    "mydholes.downh_composite(table_name='assay', \n",
    "                         variable_name = 'Au_D1', \n",
    "                         new_table_name = 'cmp_D1', \n",
    "                         cint = 2.7,                  # composite length\n",
    "                         minlen=-1,                   # minlen will be set cint/2 if <0\n",
    "                         overwrite =True)\n",
    "\n",
    "mydholes.table['cmp_D1'].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical analysis\n",
    "#### Declustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declustering parameters \n",
    "parameters_declus = { \n",
    "        'x'      :  mydholes.table['assay'].loc[mydholes.table['assay']['CMPDOM']==1, 'xm'], \n",
    "        'y'      :  mydholes.table['assay'].loc[mydholes.table['assay']['CMPDOM']==1, 'ym'],  \n",
    "        'z'      :  mydholes.table['assay'].loc[mydholes.table['assay']['CMPDOM']==1, 'zm'], \n",
    "        'vr'     :  mydholes.table['assay'].loc[mydholes.table['assay']['CMPDOM']==1, 'Au'],   \n",
    "        'anisy'  :  1.,       \n",
    "        'anisz'  :  .05,              \n",
    "        'minmax' :  0,                 \n",
    "        'ncell'  :  100,                  \n",
    "        'cmin'   :  10., \n",
    "        'cmax'   :  100.,                 \n",
    "        'noff'   :  8,                    \n",
    "        'maxcel' :  -1}               \n",
    "\n",
    "# declustering \n",
    "wtopt,vrop,wtmin,wtmax,error, \\\n",
    "xinc,yinc,zinc,rxcs,rycs,rzcs,rvrcr = pygslib.gslib.declus(parameters_declus)\n",
    "\n",
    "#Plotting declustering optimization results\n",
    "plt.plot (rxcs, rvrcr, '-o')\n",
    "plt.xlabel('X cell size')\n",
    "plt.ylabel('declustered mean')\n",
    "plt.show()\n",
    "plt.plot (rycs, rvrcr, '-o')\n",
    "plt.xlabel('Y cell size')\n",
    "plt.ylabel('declustered mean')\n",
    "plt.show()\n",
    "plt.plot (rzcs, rvrcr, '-o')\n",
    "plt.xlabel('Z cell size')\n",
    "plt.ylabel('declustered mean')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fix the cell size to 60 x 60 x 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declustering parameters \n",
    "parameters_declus = { \n",
    "        'x'      :  mydholes.table['assay'].loc[mydholes.table['assay']['CMPDOM']==1, 'xm'], \n",
    "        'y'      :  mydholes.table['assay'].loc[mydholes.table['assay']['CMPDOM']==1, 'ym'],  \n",
    "        'z'      :  mydholes.table['assay'].loc[mydholes.table['assay']['CMPDOM']==1, 'zm'], \n",
    "        'vr'     :  mydholes.table['assay'].loc[mydholes.table['assay']['CMPDOM']==1, 'Au'],   \n",
    "        'anisy'  :  1.,       \n",
    "        'anisz'  :  .05,              \n",
    "        'minmax' :  0,                 \n",
    "        'ncell'  :  1,                  \n",
    "        'cmin'   :  60., \n",
    "        'cmax'   :  60.,                 \n",
    "        'noff'   :  8,                    \n",
    "        'maxcel' :  -1} \n",
    " \n",
    "\n",
    "# declustering \n",
    "wtopt,vrop,wtmin,wtmax,error, \\\n",
    "xinc,yinc,zinc,rxcs,rycs,rzcs,rvrcr = pygslib.gslib.declus(parameters_declus)\n",
    "\n",
    "# Adding declustering weight to a drillhole interval table\n",
    "mydholes.table['assay']['declustwt'] = 1\n",
    "mydholes.table['assay'].loc[mydholes.table['assay']['CMPDOM']==1, 'declustwt'] = wtopt\n",
    "\n",
    "# calculating declustered mean\n",
    "decl_mean = rvrcr[0]\n",
    "\n",
    "print ('declustered mean:', decl_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " mydholes.table['assay'].loc[mydholes.table['assay']['CMPDOM']==1, ['BHID','FROM','TO','Au','declustwt']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical analysis\n",
    "\n",
    "#### Plots... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to plot clustered cdf \n",
    "parameters_probplt = {\n",
    "    # gslib parameters for histogram calculation  \n",
    "    'iwt'  : 0, # input boolean (Optional: set True). Use weight variable?\n",
    "    'va'   : mydholes.table['assay'].loc[mydholes.table['assay']['CMPDOM']==1, 'Au'], # input rank-1 array('d') with bounds (nd). Variable\n",
    "    'wt'   : mydholes.table['assay'].loc[mydholes.table['assay']['CMPDOM']==1, 'declustwt'], # input rank-1 array('d') with bounds (nd) (Optional, set to array of ones). Declustering weight. \n",
    "    # visual parameters for figure (if a new figure is created)\n",
    "    'figure' : None, # a bokeh figure object (Optional: new figure created if None). Set none or undefined if creating a new figure. \n",
    "    'title'  : 'Prob blot', # string (Optional, \"Histogram\"). Figure title\n",
    "    'xlabel' : 'Au', # string (Optional, default \"Z\"). X axis label \n",
    "    'ylabel' : 'P[Z<c]', # string (Optional, default \"f(%)\"). Y axis label\n",
    "    'xlog' : 1, # boolean (Optional, default True). If true plot X axis in log sale.\n",
    "    'ylog' : 1, # boolean (Optional, default True). If true plot Y axis in log sale.            \n",
    "    # visual parameter for the probplt\n",
    "    'style' : 'cross', # string with valid bokeh chart type \n",
    "    'color' : 'blue', # string with valid CSS colour (https://www.w3schools.com/colors/colors_names.asp), or an RGB(A) hex value, or tuple of integers (r,g,b), or tuple of (r,g,b,a) (Optional, default \"navy\")\n",
    "    'legend': 'Au non-declustered', # string (Optional, default \"NA\"). \n",
    "    'alpha' : 1, # float [0-1] (Optional, default 0.5). Transparency of the fill colour \n",
    "    'lwidth': 0, # float (Optional, default 1). Line width\n",
    "    # leyend\n",
    "    'legendloc': 'bottom_right'} #  float (Optional, default 'top_right'). Any of top_left, top_center, top_right, center_right, bottom_right, bottom_center, bottom_left, center_left or center\n",
    "\n",
    "# parameters to plot declustered cdf \n",
    "parameters_probplt_dcl = parameters_probplt.copy()  # make a copy!!!!\n",
    "parameters_probplt_dcl['iwt']=1                     # and update values\n",
    "parameters_probplt_dcl['legend']='Au declustered'\n",
    "parameters_probplt_dcl['color'] = 'red'\n",
    "\n",
    "# plot clustered and save plot in fig\n",
    "results_clustered, fig = pygslib.plothtml.probplt(parameters_probplt)\n",
    "\n",
    "\n",
    "# to plot one on top of the other we add figure to parameter file and plot declustered\n",
    "parameters_probplt_dcl['figure']= fig\n",
    "results_declustered, fig = pygslib.plothtml.probplt(parameters_probplt_dcl)\n",
    "\n",
    "# show the plot\n",
    "pygslib.plothtml.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also extract the values of the plots: \n",
    "print ('stats Declustered')\n",
    "print ('======================================')\n",
    "print ('CV',    results_declustered['xcvr'])\n",
    "print ('Mean', results_declustered['xmen'])\n",
    "print ('Min', results_declustered['xmin'])\n",
    "print ('Max', results_declustered['xmax'])\n",
    "print ('')\n",
    "print ('stats Clustered')\n",
    "print ('======================================')\n",
    "print ('CV',    results_clustered['xcvr'])\n",
    "print ('Mean', results_clustered['xmen'])\n",
    "print ('Min', results_clustered['xmin'])\n",
    "print ('Max', results_clustered['xmax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydholes.table['assay'].loc[mydholes.table['assay']['CMPDOM']==1, 'Au'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical analysis\n",
    "\n",
    "#### Variography\n",
    "\n",
    "For now use this variogram model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In Pygslib\n",
    "```\n",
    "vario_model = {\n",
    "            # Variogram parameters Pygslib\n",
    "            # ----------\n",
    "            'c0'         : 0.1,   \n",
    "            'it'         : [1],    # \n",
    "            'cc'         : [.9],     \n",
    "            'aa'         : [100],   \n",
    "            'aa1'        : [100],  \n",
    "            'aa2'        : [20],   \n",
    "            'ang1'       : [0],   \n",
    "            'ang2'       : [0],  \n",
    "            'ang3'       : [-15]}  \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "In gslib \n",
    "\n",
    "```\n",
    "1    0.1                      -nst, nugget effect\n",
    "1    0.9  0.0   0.0  -15.0     -it,cc,ang1,ang2,ang3\n",
    "       100.0  100.0  20.0     -a_hmax, a_hmin, a_vert\n",
    "\n",
    "\n",
    "nst and c0: the number of variogram structures and the nugget\n",
    "it:  the type of structure\n",
    "cc:  the c parameter \"sill\"\n",
    "ang1,ang2,ang3: the angles defining the geometric anisotropy\n",
    "aa: also aa_hmax, the maximum horizontal range\n",
    "aa1: also aa_hmin, the minimum horizontal range\n",
    "aa2: also aa_vert, the vertical range\n",
    "\n",
    "\n",
    "it is \n",
    "\n",
    "1. Spherical (use actual range)\n",
    "2. Exponential (use practical range)\n",
    "3. Gaussian (use practical range)\n",
    "4. Power law variogram\n",
    "5. Cosine hole effect model\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise 6 Interpolation and validation\n",
    "\n",
    "First estimate in a single block and review results, then estimate in the entire model\n",
    "\n",
    "For estimation you may use the function ``pygslib.gslib.kt3d``, which is the GSLIB’s KT3D program modified and embedded into python. KT3D now includes a maximum number of samples per drillhole in the search ellipsoid and the estimation is only in the blocks provided as arrays. \n",
    "\n",
    "The input parameters of ``pygslib.gslib.kt3d`` are defined in a large and complicated dictionary. You can get this dictionary by typing \n",
    "\n",
    "```\n",
    "print pygslib.gslib.kt3d.__doc__\n",
    "```\n",
    "\n",
    "Note that some parameters are optional. PyGSLIB will initialize those parameters to zero or to array of zeros, for example if you exclude the coordinate Z, PyGSLIB will create an array of zeros in its place.\n",
    "\n",
    "To understand GSLIB’s KT3D parameters you may read the [GSLIB user manual](https://www.amazon.ca/GSLIB-Geostatistical-Software-Library-Users/dp/0195100158) or [the kt3d gslib program parameter documentation](http://www.statios.com/help/kt3d.html). \n",
    "\n",
    "Note that in PyGSLIB the parameters nx, ny and nz are only used by superblock search algorithm, if these parameters are arbitrary the output will be correct, but the running time may be longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the parameter file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating BHID of type integer, this is to be able to use drillhole id in Fortran!\n",
    "mydholes.txt2intID('assay')\n",
    "mydholes.table[\"assay\"][['BHID','FROM','TO','BHIDint']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating parameter dictionary for estimation in one block\n",
    "kt3d_Parameters = {\n",
    "            # Input Data (Only using intervals in the mineralized domain)\n",
    "            # ----------\n",
    "            'x' : mydholes.table[\"assay\"]['xm'][mydholes.table[\"assay\"]['CMPDOM']==1].values, \n",
    "            'y' : mydholes.table[\"assay\"]['ym'][mydholes.table[\"assay\"]['CMPDOM']==1].values,\n",
    "            'z' : mydholes.table[\"assay\"]['zm'][mydholes.table[\"assay\"]['CMPDOM']==1].values,\n",
    "            'vr' : mydholes.table[\"assay\"]['Au'][mydholes.table[\"assay\"]['CMPDOM']==1].values,\n",
    "            'bhid' : mydholes.table[\"assay\"]['BHIDint'][mydholes.table[\"assay\"]['CMPDOM']==1].values, # an interger BHID\n",
    "            # Output (Target) \n",
    "            # ----------\n",
    "            'nx' : 100,  # these parameters are only used to define supperblock search\n",
    "            'ny' : 100,  \n",
    "            'nz' : 100, \n",
    "            'xmn' : 0,  \n",
    "            'ymn' : 0,  \n",
    "            'zmn' : 0,  \n",
    "            'xsiz' : 5,  \n",
    "            'ysiz' : 5,   \n",
    "            'zsiz' : 5, \n",
    "            'nxdis' : 5,  \n",
    "            'nydis' : 5,  \n",
    "            'nzdis' : 3,  \n",
    "            'outx' : mymodel.bmtable['XC'][mymodel.bmtable['IJK']==16682].values,  # filter to estimate only on block with IJK 1149229\n",
    "            'outy' : mymodel.bmtable['YC'][mymodel.bmtable['IJK']==16682].values,\n",
    "            'outz' : mymodel.bmtable['ZC'][mymodel.bmtable['IJK']==16682].values,\n",
    "            # Search parameters \n",
    "            # ----------\n",
    "            'radius'     : 60,   \n",
    "            'radius1'    : 60,   \n",
    "            'radius2'    : 8,   \n",
    "            'sang1'      : 0,  \n",
    "            'sang2'      : 0,   \n",
    "            'sang3'      : -15,   \n",
    "            'ndmax'      : 20,    \n",
    "            'ndmin'      : 7,  \n",
    "            'noct'       : 0,\n",
    "            'nbhid'      : 5,   \n",
    "            # Kriging parameters and options \n",
    "            # ----------\n",
    "            'ktype'      : 1,   # 1 Ordinary kriging \n",
    "            'idbg'       : 1,   # 0 no debug \n",
    "            # Variogram parameters Pygslib\n",
    "            # ----------\n",
    "            'c0'         : 0.1,   \n",
    "            'it'         : [1],    \n",
    "            'cc'         : [.9],     \n",
    "            'aa'         : [100],   \n",
    "            'aa1'        : [100],  \n",
    "            'aa2'        : [20],   \n",
    "            'ang1'       : [0],   \n",
    "            'ang2'       : [0],  \n",
    "            'ang3'       : [-15]}   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the parameters in one block\n",
    "\n",
    "Only the block with index *IJK* equal to 1149229 was used this time and ``'idbg'`` was set to one in order to get a full output of the last (and unique) block estimate, including the samples selected, kriging weight and the search ellipsoid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating in one block\n",
    "estimate, debug, summary = pygslib.gslib.kt3d(kt3d_Parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can export the results to Paraview to better observe the results of the estimate in a single block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving debug to a csv file using Pandas\n",
    "pd.DataFrame({'x':debug['dbgxdat'],'y':debug['dbgydat'],'z':debug['dbgzdat'],'wt':debug['dbgwt']}).to_csv('dbg_data.csv', index=False)\n",
    "\n",
    "# save the search ellipse to a VTK file\n",
    "pygslib.vtktools.SavePolydata(debug['ellipsoid'], 'search_ellipsoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may look like this\n",
    "\n",
    "<img src = 'figures/fig8.JPG' widht = '40%' heigth = '40%'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate block variance, wee need it for global change of support validation\n",
    "# you can also calculate this with the function pygslib.gslib.block_covariance(...)\n",
    "cbb=debug['cbb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate in the entire block model\n",
    "After testing the estimation parameters in few blocks you may be ready to estimate in all the blocks within the mineralized domain. Just update the parameter file to remove the debug option and reassign the target coordinates as the actual blocks coordinate arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update parameter file\n",
    "kt3d_Parameters['idbg'] = 0 # set the debug of\n",
    "kt3d_Parameters['outx'] = mymodel.bmtable['XC'].values  # use all the blocks \n",
    "kt3d_Parameters['outy'] = mymodel.bmtable['YC'].values\n",
    "kt3d_Parameters['outz'] = mymodel.bmtable['ZC'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimating in all blocks\n",
    "estimate, debug, summary = pygslib.gslib.kt3d(kt3d_Parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the estimate into the model\n",
    "mymodel.bmtable['Au_OK'] = estimate['outest']\n",
    "mymodel.bmtable['Au_ID2'] = estimate['outidpower']\n",
    "mymodel.bmtable['Au_NN'] = estimate['outnn']\n",
    "mymodel.bmtable['Au_Lagrange'] = estimate['outlagrange']\n",
    "mymodel.bmtable['Au_KVar']= estimate['outkvar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting block model to VTK (unstructured grid) \n",
    "mymodel.blocks2vtkUnstructuredGrid(path='model.vtu')\n",
    "\n",
    "# exporting to csv using Pandas\n",
    "mymodel.bmtable['Domain']= 1\n",
    "mymodel.bmtable[mymodel.bmtable['Au_OK'].notnull()].to_csv('model.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Validations\n",
    "\n",
    "Basic validations are:\n",
    "\n",
    " - visual validation\n",
    " - comparison of mean grades\n",
    " - swath plots \n",
    " - global change of support (GCOS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual validations \n",
    "Open the model and drillholes in Paraview and inspect: \n",
    "\n",
    "- blocks non-estimated\n",
    "- reproduction of trends and estimation artifacts\n",
    "- similarity between interpolated grade and nearby composites/assays\n",
    "\n",
    "This is an example\n",
    "\n",
    "<img src = 'figures/fig9.JPG' width = '70%' > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Mean in model OK   :\",  mymodel.bmtable['Au_OK'].mean())\n",
    "print (\"Mean in model ID2   :\",  mymodel.bmtable['Au_ID2'].mean())\n",
    "print (\"Mean in model NN   :\",  mymodel.bmtable['Au_NN'].mean())\n",
    "print (\"Mean in data    :\", mydholes.table[\"assay\"]['Au'][mydholes.table[\"assay\"]['CMPDOM']==1].mean())\n",
    "print (\"Declustered mean:\", decl_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create swath plots\n",
    "\n",
    "There are two ways of doing swath plots\n",
    "\n",
    "-\tSlicing block model and data and comparing the declustered means of each slice\n",
    "-\tCalculating nearest neighbor in blocks (this is equivalent to declustered values) and comparing means of nearest neighbor estimates with means of other estimation methods along row, columns and levels.  \n",
    "\n",
    "We do not have a function in pygslib to do that, but we can implement the second option with one line of ``pandas``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.bmtable.groupby('XC')[['Au_OK','Au_ID2','Au_NN']].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.bmtable.groupby('YC')[['Au_OK','Au_ID2','Au_NN']].mean().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.bmtable.groupby('ZC')[['Au_OK','Au_ID2','Au_NN']].mean().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global change of support\n",
    "The process is: \n",
    "a) fit point anamorphosis function\n",
    "b) calculate the support correction coefficient r for the block size you have in your model\n",
    "c) fit block anamorphosis\n",
    "d) use the anamorphosis to calculate grade tonnage and compare it with grade tonnage generated with the block model\n",
    "\n",
    "You have to fit the anamorphosis function, this is easy, normally no teak is required...  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit anamorphosis by changing, zmax, zmin, and extrapolation function\n",
    "PCI, H, raw, zana, gauss, z, P, raw_var, PCI_var, fig1 = pygslib.nonlinear.anamor(\n",
    "                         z = mydholes.table[\"assay\"].loc[mydholes.table['assay']['CMPDOM']==1, 'Au'], \n",
    "                         w = mydholes.table[\"assay\"].loc[mydholes.table['assay']['CMPDOM']==1, 'declustwt'], \n",
    "                         zmin = mydholes.table[\"assay\"].loc[mydholes.table['assay']['CMPDOM']==1, 'Au'].min(), \n",
    "                         zmax = mydholes.table[\"assay\"].loc[mydholes.table['assay']['CMPDOM']==1, 'Au'].max(),\n",
    "                         zpmin = None, zpmax = None,\n",
    "                         ymin=-5, ymax=5,\n",
    "                         ndisc = 5000,\n",
    "                         ltail=1, utail=4, ltpar=1, utpar=1.5, K=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the support correction coefficient r\n",
    "r = pygslib.nonlinear.get_r(Var_Zv = cbb, PCI = PCI)\n",
    "\n",
    "print ('cbb :', cbb)\n",
    "print ('r   :', r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit block anamorphosis\n",
    "ZV, PV, fig2 = pygslib.nonlinear.anamor_blk( PCI, H, r = r, gauss = gauss, Z = z,\n",
    "                  ltail=1, utail=1, ltpar=1, utpar=1,\n",
    "                  raw=raw, zana=zana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate grade tonnage courve\n",
    "\n",
    "cutoff = np.arange(0, 3, 0.01)\n",
    "tt = []\n",
    "gg = []\n",
    "label = []\n",
    "\n",
    "# calculate GTC from gaussian in block support \n",
    "t,ga,gb = pygslib.nonlinear.gtcurve (cutoff = cutoff, z=ZV, p=PV, varred = 1, ivtyp = 0, zmin = 0, zmax = None,\n",
    "             ltail = 1, ltpar = 1, middle = 1, mpar = 1, utail = 1, utpar = 1,maxdis = 1000)\n",
    "tt.append(t)\n",
    "gg.append(ga)\n",
    "label.append('DGM with block support')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how it look with GCOS. This is also know as global estimation\n",
    "fig = pygslib.nonlinear.plotgt(cutoff = cutoff, t = tt, g = gg, label = label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to compare global resources with the one estimated we calculate the CDF of the blocks\n",
    "\n",
    "# cdf of kriging estimate\n",
    "parameters_probplt = {\n",
    "        'iwt'  : 0,                             #int, 1 use declustering weight\n",
    "        'va'   : mymodel.bmtable['Au_OK'][mymodel.bmtable['Au_OK'].notnull()].values,             # array('d') with bounds (nd)\n",
    "        'wt'   : np.ones(mymodel.bmtable['Au_OK'][mymodel.bmtable['Au_OK'].notnull()].shape[0])} # array('d') with bounds (nd), wight variable (obtained with declust?)\n",
    "\n",
    "\n",
    "binval_ok,cl_ok,xpt025,xlqt,xmed,xuqt,xpt975,xmin,xmax, \\\n",
    "xcvr,xmen,xvar,error = pygslib.gslib.__plot.probplt(**parameters_probplt)\n",
    "\n",
    "# cdf of id2\n",
    "parameters_probplt = {\n",
    "        'iwt'  : 0,                             #int, 1 use declustering weight\n",
    "        'va'   : mymodel.bmtable['Au_ID2'][mymodel.bmtable['Au_OK'].notnull()].values,             # array('d') with bounds (nd)\n",
    "        'wt'   : np.ones(mymodel.bmtable['Au_OK'][mymodel.bmtable['Au_OK'].notnull()].shape[0])} # array('d') with bounds (nd), wight variable (obtained with declust?)\n",
    "\n",
    "binval_id2,cl_id2,xpt025,xlqt,xmed,xuqt,xpt975,xmin,xmax, \\\n",
    "xcvr,xmen,xvar,error = pygslib.gslib.__plot.probplt(**parameters_probplt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate GTC ok \n",
    "t,ga,gb = pygslib.nonlinear.gtcurve (cutoff = cutoff, z=cl_ok, p=binval_ok, varred = 1, ivtyp = 2, zmin = 0, zmax = None,\n",
    "             ltail = 1, ltpar = 1, middle = 1, mpar = 1, utail = 1, utpar = 1,maxdis = 1000)\n",
    "tt.append(t)\n",
    "gg.append(ga)\n",
    "label.append('Ordinary Kriging')\n",
    "\n",
    "# calculate GTC in block support\n",
    "t,ga,gb = pygslib.nonlinear.gtcurve (cutoff = cutoff, z=cl_id2, p=binval_id2, varred = 1, ivtyp = 2, zmin = 0, zmax = None,\n",
    "             ltail = 1, ltpar = 1, middle = 1, mpar = 1, utail = 1, utpar = 1,maxdis = 1000)\n",
    "tt.append(t)\n",
    "gg.append(ga)\n",
    "label.append('Inverse of the Distance 2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pygslib.nonlinear.plotgt(cutoff = cutoff, t = tt, g = gg, label = label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can plot differences (relative error in grade)\n",
    "plt.plot (cutoff, gg[0]-gg[1], label = 'DGM - OK')\n",
    "plt.plot (cutoff, gg[0]-gg[2], label = 'DGM - ID2')\n",
    "plt.plot (cutoff, np.zeros(cutoff.shape[0]),'--k', label = 'Zero error')\n",
    "plt.title('relative error in grade')\n",
    "plt.xlabel ('cutoff')\n",
    "plt.ylabel ('relative error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can plot differences (relative error in tonnage)\n",
    "plt.plot (cutoff, tt[0]-tt[1], label = 'DGM - OK')\n",
    "plt.plot (cutoff, tt[0]-tt[2], label = 'DGM - ID2')\n",
    "plt.plot (cutoff, np.zeros(cutoff.shape[0]),'--k', label = 'Zero error')\n",
    "plt.legend()\n",
    "plt.xlabel ('cutoff')\n",
    "plt.ylabel ('relative error')\n",
    "plt.title('relative error in tonnage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
